{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_I9MJ5ZQnc5h"
      },
      "source": [
        "\n",
        "# Installing and importing dependencies\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDFpQUFZBBdF"
      },
      "outputs": [],
      "source": [
        "!pip install attention\n",
        "!pip install movecolumn\n",
        "!pip install tensorflow_addons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BrSvQfzQ_T4B"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import datetime\n",
        "import os,json\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt\n",
        "import string\n",
        "import numpy as np\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.models import Model\n",
        "from sklearn.utils import shuffle\n",
        "import statistics\n",
        "import keras,tensorflow\n",
        "from keras.layers import GRU, Flatten\n",
        "from attention import Attention\n",
        "from keras.layers import LSTM, Input, TimeDistributed, Dense, Activation, RepeatVector, Embedding\n",
        "from keras.optimizers import Adam\n",
        "from datetime import datetime, timedelta\n",
        "from math import sqrt\n",
        "from numpy import concatenate\n",
        "from matplotlib import pyplot\n",
        "from pandas import read_csv\n",
        "from pandas import DataFrame\n",
        "from pandas import concat\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "import tensorflow as tf\n",
        "import movecolumn as mc\n",
        "from keras.layers import Bidirectional \n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers import Dropout\n",
        "from tensorflow_addons.optimizers import AdamW\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import pickle\n",
        "import os\n",
        "from tensorflow.keras.regularizers import L1L2\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras import Input, Model \n",
        "from tensorflow.keras.layers import * \n",
        "from tensorflow.keras import backend \n",
        "from tensorflow.keras import utils\n",
        "backend.clear_session()\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYuNkqzdnk5R"
      },
      "source": [
        "# Backend procesing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dPkvINZYBDVu"
      },
      "outputs": [],
      "source": [
        "\n",
        "def plot_autocorr(df):\n",
        "    f, ax = plt.subplots(figsize=(10, 8))\n",
        "    corr = df.corr()\n",
        "    sns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True),\n",
        "                square=True, ax=ax,annot=True)\n",
        "def read_dataframe(link):\n",
        "  return pd.read_csv(link)\n",
        "def minmaxavg(df):\n",
        "\n",
        "  stats = df.groupby('dates')['NumArticles'].agg(['mean', 'max', 'min','count'])\n",
        "  stats.columns = ['mean_NumArticles', 'max_NumArticles', 'min_NumArticles','NumberEvents']\n",
        "\n",
        "\n",
        "  stats2 = df.groupby('dates')['GoldsteinScale'].agg(['mean', 'max', 'min',])\n",
        "  stats2.columns = ['mean_GoldsteinScale', 'max_GoldsteinScale', 'min_GoldsteinScale',]\n",
        "\n",
        "  stats = stats2.merge(stats, left_on = 'dates', right_index=True, how = 'left')\n",
        "  \n",
        "  stats3 = df.groupby('dates')['AvgTone'].agg(['mean', 'max', 'min'])\n",
        "  stats3.columns = ['mean_AvgTone', 'max_AvgTone', 'min_AvgTone']\n",
        "  \n",
        "  stats = stats3.merge(stats, left_on = 'dates', right_index=True, how = 'left')\n",
        "  \n",
        "\n",
        " \n",
        "  stats4 = df.groupby('dates')['IsRootEvent'].agg(['mean', 'max', 'min','count'])\n",
        "  stats4.columns = ['mean_IsRootEvent', 'max_IsRootEvent', 'min_IsRootEvent',\"count_IsRootEvent\"]\n",
        "  stats = stats4.merge(stats, left_on = 'dates', right_index=True, how = 'left')\n",
        "  \n",
        "  stats4 = df.groupby('dates')['positive'].agg(['mean', 'max', 'min','count'])\n",
        "  stats4.columns = ['mean_positive', 'max_positive', 'min_positive',\"count_positive\"]\n",
        "  stats = stats4.merge(stats, left_on = 'dates', right_index=True, how = 'left')\n",
        "\n",
        "  stats5 = df.groupby('dates')['negative'].agg(['mean', 'max', 'min','count'])\n",
        "  stats5.columns = ['mean_negative', 'max_negative', 'min_negative',\"count_negative\"]\n",
        "  stats = stats5.merge(stats, left_on = 'dates', right_index=True, how = 'left')\n",
        "\n",
        "  stats6 = df.groupby('dates')['neutral'].agg(['mean', 'max', 'min','count'])\n",
        "  stats6.columns = ['mean_neutral', 'max_neutral', 'min_neutral',\"count_neutral\"]\n",
        "  stats = stats6.merge(stats, left_on = 'dates', right_index=True, how = 'left')\n",
        "\n",
        "  lone_stats = stats\n",
        "  gold = lone_stats['mean_NumArticles'].tolist()\n",
        "  tone = lone_stats['mean_AvgTone'].tolist()\n",
        "  \n",
        "  res = []\n",
        "  for i,j in zip(gold,tone):\n",
        "    try:\n",
        "      result = i / j\n",
        "    except ZeroDivisionError:\n",
        "      result = 0\n",
        "   \n",
        "    res.append(result)\n",
        "\n",
        "  lone_stats = lone_stats.reset_index(level=0)\n",
        "  lone_stats['articles_ratio_tone'] = res\n",
        " \n",
        "  list_dates = lone_stats['dates'].tolist()\n",
        "  last_week = []\n",
        "  for day in list_dates:\n",
        "    last_week.append(len(lone_stats[lone_stats['dates'] > day - pd.to_timedelta(\"7day\")]))\n",
        "  \n",
        "  lone_stats['last_week'] = last_week\n",
        "  last_week = []\n",
        "  for day in list_dates:\n",
        "    last_week.append(len(lone_stats[lone_stats['dates'] > day - pd.to_timedelta(\"14day\")]))\n",
        "\n",
        "  lone_stats['last_2weeks'] = last_week\n",
        "\n",
        "  last_week = []\n",
        "  for day in list_dates:\n",
        "    last_week.append(len(lone_stats[lone_stats['dates'] > day - pd.to_timedelta(\"30day\")]))\n",
        "\n",
        "  lone_stats['last_month'] = last_week\n",
        "\n",
        " \n",
        "\n",
        "  \n",
        "  return stats,lone_stats\n",
        "\n",
        "def count_sentiment(df):\n",
        "  counting_df = pd.DataFrame()\n",
        "  list_dates = df['dates'].tolist()\n",
        "  topic_list = set(df['feelings'].tolist())\n",
        "  \n",
        "  listt = []\n",
        "  point = 0\n",
        "  for i in range(3):\n",
        "    listt.append([])\n",
        "\n",
        "\n",
        " \n",
        "  for day in list_dates:\n",
        "    daily = df.loc[df['dates'] == day]\n",
        "   \n",
        "    for i in [0,1,2]:\n",
        "   \n",
        "      partial = daily.loc[daily['feelings'] == i]\n",
        "    \n",
        "      listt[i].append(len(partial))\n",
        "    point += 1\n",
        "\n",
        "  for i  in range(len(listt)):\n",
        "    counting_df[\"feelings\"+str(i)] = listt[i]\n",
        "\n",
        "  counting_df[\"dates\"] = list_dates\n",
        "  return counting_df\n",
        "\n",
        "def get_topics(df):\n",
        "\n",
        "  cats = ['Social',\"Política\",'Migración / Racismo','Salud','Otros']\n",
        "  cat = df['categories'].tolist()\n",
        "  print(cat)\n",
        "  per = []\n",
        "  topics = []\n",
        "  for dic in cat:\n",
        "\n",
        "    res = list(eval(dic))\n",
        "    per.append(res)\n",
        "\n",
        "  list_topics = []\n",
        "  for a in per:\n",
        "    try:\n",
        "\n",
        "      list_topics.append(a[0]['name'])\n",
        "    except:\n",
        "      list_topics.append(\"\")\n",
        "  return list_topics\n",
        "\n",
        "def process_list(ls):\n",
        "  final = []\n",
        "  for l in ls:\n",
        "    if 'Pol' in l:\n",
        "      final.append(\"Politica\")\n",
        "    elif 'Salud' in l:\n",
        "      final.append(\"Salud\")\n",
        "    elif 'Migración' in l:\n",
        "       final.append(\"Migracion\")\n",
        "    elif 'Social' in l:\n",
        "       final.append(\"Social\")\n",
        "    else:\n",
        "      final.append(\"otros\")\n",
        "  return final\n",
        "\n",
        "\n",
        "def group_by_topic(df):\n",
        "  dfs = []\n",
        "  topics = set(df['lista_topics'].tolist())\n",
        "  for top in topics:\n",
        "    dfs.append(df[df['lista_topics'] == top])\n",
        "  return dfs\n",
        "\n",
        "def group_titles_by_day(df):\n",
        "  days = df['dates'].tolist()\n",
        "  days = set(days)\n",
        "  titles_matrix = []\n",
        "  for day in days:\n",
        "\n",
        "    partial = df[df['dates'] == day]\n",
        "    titles_matrix.append(partial['titles'].tolist())\n",
        "  return titles_matrix \n",
        "\n",
        "\n",
        "def group_titles_by_day(dates,df):\n",
        "  days = set(dates)\n",
        "  titles_matrix = []\n",
        "  for day in days:\n",
        "    partial = df[df['dates'] == day]\n",
        "    titles_matrix.append(partial['titles'].tolist())\n",
        "  return titles_matrix \n",
        "  \n",
        "def calculate_rolling(df):\n",
        "  keys = df.keys()\n",
        "  days = [3,5,7,10,14,30,60,90]\n",
        "\n",
        "  for key in keys:\n",
        "      try:\n",
        "        for day in days:\n",
        "          key_df = key+\"_\"+str(day)\n",
        "          df[key_df] = df[key].rolling(day).mean()\n",
        "      except:\n",
        "        continue\n",
        "  return df\n",
        "\n",
        "def count_fakes_in_day(df,df_maldita):\n",
        "  dates = df['dates'].tolist()\n",
        "  len_dates = []\n",
        "  for date in dates:\n",
        "  \n",
        "    len_dates.append(len(df_maldita[df_maldita['just_date'] == str(date).split(\" \")[0] ]))\n",
        "  \n",
        "  df['number_fake_news'] = len_dates\n",
        "\n",
        "  return df\n",
        "\n",
        "\n",
        "\n",
        "def create_dataset(df,days,flag):\n",
        "  \n",
        "  list_dates = df['dates'].tolist()\n",
        "  quantity_fakes = df['number_fake_news'].tolist()\n",
        "  reached_first = flag\n",
        "  pointer = 0\n",
        "  target_values = []\n",
        "  dataset = []\n",
        "  for date in list_dates:\n",
        "    if reached_first == 1:\n",
        "      d = date - timedelta(days=days)\n",
        "      str_d = str(d).split(\" \")[0]\n",
        "      partial_df = df[(df['dates'] <= date) & (df['dates'] >= str_d)]\n",
        "      get_val = df[(df['dates'] == date)]\n",
        "      target_size = days + 1\n",
        "      partial_df = partial_df.drop(columns=['dates','number_fake_news','just_dates'])\n",
        "      if len(partial_df.values.tolist()) == target_size:\n",
        "        dataset.append(partial_df.values.tolist())\n",
        "        target_values.append(get_val['number_fake_news'].tolist()[0])\n",
        "    else:\n",
        "      if quantity_fakes[pointer] > 1000:\n",
        "        reached_first = 1\n",
        "      pointer += 1\n",
        "\n",
        "  return dataset,target_values,list_dates\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def prepare_data_consistency(dataset,target_values,scaler):\n",
        "  \n",
        "  train = dataset\n",
        "  train_X = np.asarray(train).astype(np.float32)\n",
        "  train_y = np.asarray(target_values).astype(np.float32)\n",
        "\n",
        "  return train_X,train_y\n",
        "\n",
        "def prepare_data(dataset,target_values,list_days,percentage):\n",
        "\n",
        "  train = dataset\n",
        "  train_X = np.asarray(train).astype(np.float32)\n",
        "  train_y = np.asarray(target_values).astype(np.float32)\n",
        "  diff_days = abs(len(train_X)-len(list_days))\n",
        "  list_days = list_days[:len(list_days)-diff_days]\n",
        "  X_test = train_X[int(len(train_X)*percentage):] \n",
        "  X_train= train_X[:int(len(train_X)*percentage)]\n",
        "  y_test = train_y[int(len(train_y)*percentage):]\n",
        "  y_train = train_y[:int(len(train_y)*percentage)]\n",
        "\n",
        "\n",
        "  return X_test,X_train,y_test,y_train,scaler\n",
        "\n",
        "\n",
        "\n",
        "def count_fakes_in_day(df,df_maldita):\n",
        "  dates = df['dates'].tolist()\n",
        "  len_dates = []\n",
        "  for date in dates:\n",
        "  \n",
        "    len_dates.append(len(df_maldita[df_maldita['just_date'] == str(date).split(\" \")[0] ]))\n",
        "  \n",
        "  \n",
        "  df['number_fake_news'] = len_dates\n",
        "\n",
        "  return df\n",
        "  \n",
        "def count_fakes_in_next_days(df,df_maldita,days):\n",
        "  dates = df['dates'].tolist()\n",
        "  len_dates = []\n",
        "  for date in dates:    \n",
        "    d = date + timedelta(days=days)\n",
        "\n",
        "    str_d = str(d).split(\" \")[0]\n",
        "    original_date = str(date).split(\" \")[0]\n",
        "\n",
        "    testing_week = df_maldita[(df_maldita['just_date'] >= original_date) & (df_maldita['just_date'] < str_d)]\n",
        "\n",
        "    len_dates.append(len(testing_week))\n",
        "  \n",
        "  df['number_fake_news'] = len_dates\n",
        "\n",
        "  return df\n",
        "\n",
        "def group_titles_by_day(dates,df):\n",
        "  days = set(dates)\n",
        "  \n",
        "  titles_matrix = []\n",
        "  for day in days:\n",
        "    str_day = str(day).split(\" \")[0]\n",
        "    \n",
        "    partial = df[df['dates'] == str_day]\n",
        "    titles_matrix.append(partial['titles'].tolist())\n",
        "  return titles_matrix \n",
        "\n",
        "\n",
        "\n",
        "def create_dataset_target_days(df,days):\n",
        "\n",
        "  \n",
        "  list_dates = df['dates'].tolist()\n",
        "  quantity_fakes = df['number_fake_news'].tolist()\n",
        "\n",
        "  reached_first = 1\n",
        "  pointer = 0\n",
        "  target_values = []\n",
        "  dataset = []\n",
        "\n",
        "  #if len(df) < 50:\n",
        "  # reached_first = 1\n",
        "  for date in list_dates:\n",
        "\n",
        "    if reached_first == 1:\n",
        "      \n",
        "\n",
        "      d = date - timedelta(days=days)\n",
        "      str_d = str(d).split(\" \")[0]\n",
        "      partial_df = df[(df['dates'] <= date) & (df['dates'] >= str_d)]\n",
        "      get_val = df[(df['dates'] == date)]\n",
        "      \n",
        "      target_size = days + 1\n",
        "      \n",
        "      partial_df = partial_df.drop(columns=['dates','number_fake_news'])\n",
        "      \n",
        "      if len(partial_df.values.tolist()) == target_size:\n",
        "        dataset.append(partial_df.values.tolist())\n",
        "        target_values.append(get_val['number_fake_news'].tolist()[0])\n",
        "\n",
        "    else:\n",
        "      if quantity_fakes[pointer] > 1000:\n",
        "        reached_first = 1\n",
        "      pointer += 1\n",
        "\n",
        "\n",
        "  return dataset,target_values,list_dates\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTrKDgofsk0J"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_basic_LSRM(train_X,train_y):\n",
        "  \n",
        "  model2 = Sequential()\n",
        "  model2.add(LSTM(128,return_sequences=False,input_shape=(train_X.shape[1],train_X.shape[2])))\n",
        "  model2.add(Dense(16))\n",
        "  model2.add(Dense(8))\n",
        "  model2.add(Dense(len(train_y[0]),activation='linear'))\n",
        "\n",
        "  model2.compile(loss='mean_squared_error', optimizer='adam')\n",
        "  return model2"
      ],
      "metadata": {
        "id": "X-Opazpmg448"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRM4nSDZb6Or"
      },
      "outputs": [],
      "source": [
        "def autoencoder(train_X,train_y):\n",
        "\n",
        "    enc_inputs = Input(shape=(train_X.shape[1], train_X.shape[2]), name='inputs')\n",
        "\n",
        "    lstm = keras.layers.LSTM(units=64,return_sequences=True)(enc_inputs)\n",
        "    lstm = keras.layers.LSTM(units=8,return_sequences=False)(lstm)\n",
        "    gruencode = keras.layers.RepeatVector(train_X.shape[1], name='encoder_decoder_bridge')(lstm)\n",
        "    lstm = keras.layers.LSTM(units=8,return_sequences=True)(gruencode)\n",
        "    lstm = keras.layers.LSTM(units=64,return_sequences=False)(lstm)\n",
        "    dense = keras.layers.Dense(units=32, activation='linear')(lstm)\n",
        "    dense = Dropout(0.1)(dense)\n",
        "    dense = keras.layers.Dense(units=16, activation='linear')(dense)\n",
        "    dense = Dropout(0.1)(dense)\n",
        "    dense = keras.layers.Dense(units=8, activation='linear')(dense)\n",
        "    dense = Dropout(0.1)(dense)\n",
        "    dense = keras.layers.Dense(units=len(train_y[0]), activation='linear')(dense)\n",
        "    \n",
        "    model = Model(enc_inputs, dense, name='encoder')\n",
        "    model.compile(optimizer='adam',\n",
        "              loss='mean_squared_error',\n",
        "              metrics=['mse'])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_basic_model_enc_attention(train_X,train_y):\n",
        "    n_hidden = 128\n",
        "    enc_inputs = Input(shape=(train_X.shape[1], train_X.shape[2]), name='inputs')\n",
        "    encoder_last_h1 = LSTM(\n",
        "    n_hidden, dropout=0.2, recurrent_dropout=0.2, \n",
        "    return_sequences=True, return_state=False)(enc_inputs)\n",
        "    x = Attention(units=64)(encoder_last_h1)\n",
        "    mlp = Dense(64)(x)\n",
        "    mlp = Dense(32)(mlp)\n",
        "    mlp = Dense(16)(mlp)\n",
        "    mlp = Dense(8)(mlp)\n",
        "    mlp = Dense(len(train_y[0]))(mlp)\n",
        "    model = Model(enc_inputs, mlp, name='encoder')\n",
        "    opt = Adam(lr=0.01, clipnorm=1)\n",
        "    model.compile(loss='mse', optimizer='adam', metrics=['mae'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "1wMw9vOXggU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Headlines model"
      ],
      "metadata": {
        "id": "X4iy0sY7hL2K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model_text(X_train,vectorize_layersfinal,train_y):\n",
        "\n",
        "    enc_inputs = Input(shape=(X_train.shape[1], X_train.shape[2]), name='inputs')\n",
        "      \n",
        "    num_features = 10\n",
        "    embedding_dim = 128\n",
        "\n",
        "    texts_input1 = Input(shape=(1,), dtype=tf.string)\n",
        "    texts_input2 = Input(shape=(1,), dtype=tf.string)\n",
        "    texts_input3 = Input(shape=(1,), dtype=tf.string)\n",
        "    texts_input4 = Input(shape=(1,), dtype=tf.string)\n",
        "    texts_input5 = Input(shape=(1,), dtype=tf.string)\n",
        "    texts_input6 = Input(shape=(1,), dtype=tf.string)\n",
        "    texts_input7 = Input(shape=(1,), dtype=tf.string)\n",
        "    texts_input8 = Input(shape=(1,), dtype=tf.string)\n",
        "    texts_input9 = Input(shape=(1,), dtype=tf.string)\n",
        "    texts_input10 = Input(shape=(1,), dtype=tf.string)\n",
        "\n",
        "    vect1 = vectorize_layersfinal(texts_input1)\n",
        "    vect2 = vectorize_layersfinal(texts_input2)\n",
        "    vect3 = vectorize_layersfinal(texts_input3)\n",
        "    vect4 = vectorize_layersfinal(texts_input4)\n",
        "    vect5 = vectorize_layersfinal(texts_input5)\n",
        "    vect6 = vectorize_layersfinal(texts_input6)\n",
        "    vect7 = vectorize_layersfinal(texts_input7)\n",
        "    vect8 = vectorize_layersfinal(texts_input8)\n",
        "    vect9 = vectorize_layersfinal(texts_input9)\n",
        "    vect10 = vectorize_layersfinal(texts_input10)\n",
        "\n",
        "\n",
        "    test_emb = Embedding(10000, embedding_dim, name = 'embedding1')\n",
        "    emb1 = test_emb(vect1)  \n",
        "    emb2 = test_emb(vect2)  \n",
        "    emb3 = test_emb(vect3)  \n",
        "    emb4 = test_emb(vect4)  \n",
        "    emb5 = test_emb(vect5)  \n",
        "    emb6 = test_emb(vect6)  \n",
        "    emb7 = test_emb(vect7)  \n",
        "    emb8 = test_emb(vect8)  \n",
        "    emb9 = test_emb(vect9)  \n",
        "    emb10 = test_emb(vect10)  \n",
        "\n",
        "    msg_out1 = LSTM(32, kernel_regularizer = L1L2(l1=0.001, l2=0.01),dropout=0.3,return_sequences=False)(emb1)    \n",
        "    msg_out2 = LSTM(32, kernel_regularizer = L1L2(l1=0.001, l2=0.01),dropout=0.3,return_sequences=False)(emb2)    \n",
        "    msg_out3 = LSTM(32, kernel_regularizer = L1L2(l1=0.001, l2=0.01),dropout=0.3,return_sequences=False)(emb3)    \n",
        "    msg_out4 = LSTM(32, kernel_regularizer = L1L2(l1=0.001, l2=0.01),dropout=0.3,return_sequences=False)(emb4)    \n",
        "    msg_out5 = LSTM(32, kernel_regularizer = L1L2(l1=0.001, l2=0.01),dropout=0.3,return_sequences=False)(emb5)   \n",
        "    msg_out6 = LSTM(32, kernel_regularizer = L1L2(l1=0.001, l2=0.01),dropout=0.3,return_sequences=False)(emb6)    \n",
        "    msg_out7 = LSTM(32, kernel_regularizer = L1L2(l1=0.001, l2=0.01),dropout=0.3,return_sequences=False)(emb7)    \n",
        "    msg_out8 = LSTM(32, kernel_regularizer = L1L2(l1=0.001, l2=0.01),dropout=0.3,return_sequences=False)(emb8)    \n",
        "    msg_out9 = LSTM(32, kernel_regularizer = L1L2(l1=0.001, l2=0.01),dropout=0.3,return_sequences=False)(emb9)    \n",
        "    msg_out10 = LSTM(32, kernel_regularizer = L1L2(l1=0.001, l2=0.01),dropout=0.3,return_sequences=False)(emb10)     \n",
        "\n",
        "    gruencode = LSTM(256 ,return_sequences=False)(enc_inputs)\n",
        "    x = concatenate([msg_out1,msg_out2,msg_out3,msg_out4,msg_out5,msg_out6,msg_out7,msg_out8,msg_out9,msg_out10, gruencode]) \n",
        "    mlp = Dense(1024, activation='linear')(x)\n",
        "    mlp = Dropout(0.5)(mlp)\n",
        "    mlp = Dense(512, activation='linear')(mlp)\n",
        "    mlp = Dropout(0.3)(mlp)\n",
        "    mlp = Dense(256, activation='linear')(mlp)\n",
        "    mlp = Dropout(0.2)(mlp)\n",
        "    mlp = Dense(128, activation='linear')(mlp)\n",
        "    mlp = Dropout(0.2)(mlp)\n",
        "    mlp = Dense(64, activation='linear')(mlp)\n",
        "    mlp = Dropout(0.1)(mlp)\n",
        "    mlp = Dense(32, activation='linear')(mlp)\n",
        "    mlp = Dropout(0.1)(mlp)\n",
        "    mlp = Dense(16, activation='linear')(mlp)\n",
        "    mlp = Dropout(0.1)(mlp)\n",
        "    mlp = Dense(len(train_y[0]), activation='linear',kernel_constraint=tf.keras.constraints.non_neg())(mlp)\n",
        "\n",
        "    model = Model([enc_inputs,texts_input1,texts_input2,texts_input3,texts_input4,texts_input5,texts_input6,texts_input7,texts_input8,texts_input9,texts_input10], mlp, name='encoder')\n",
        "    \n",
        "    model.compile(loss='mse', optimizer='adam')\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "Bemm8Hcah8-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_gdelt = read_dataframe(\"/xxxx.csv\")\n",
        "training_maldita = read_dataframe(\"xxxxx.csv\")\n",
        "gtopL = get_topics(training_maldita)\n",
        "gtopL = process_list(gtopL)\n",
        "training_maldita['lista_topics'] = gtopL\n",
        "training_gdelt = training_gdelt.drop_duplicates(subset=[\"titles\"], keep=False)\n",
        "training_gdelt = training_gdelt.drop(columns=['Unnamed: 0', 'Unnamed: 0.1', 'Unnamed: 0.1.1', 'GLOBALEVENTID',\n",
        "      'FractionDate', 'Actor1Code',\n",
        "       'Actor1CountryCode', 'Actor1KnownGroupCode', 'Actor1EthnicCode',\n",
        "       'Actor1Religion1Code', 'Actor1Religion2Code', 'Actor1Type1Code',\n",
        "       'Actor1Type2Code', 'Actor1Type3Code', 'Actor2Type1Code',\n",
        "       'Actor2Type2Code', 'Actor2Type3Code', \n",
        "       'Actor1Geo_FeatureID', 'Actor2Geo_Type', 'Actor2Geo_FullName',\n",
        "       'Actor2Geo_CountryCode', 'Actor2Geo_ADM1Code', 'Actor2Geo_Lat',\n",
        "       'Actor2Geo_Long', 'Actor2Geo_FeatureID', 'ActionGeo_Type',\n",
        "       'ActionGeo_FullName', 'ActionGeo_CountryCode', 'ActionGeo_ADM1Code',\n",
        "       'ActionGeo_Lat', 'ActionGeo_Long', 'ActionGeo_FeatureID', \n",
        "       'SOURCEURL', 'Actor1Geo_ADM2Code', 'Actor2_ADM1Code', 'Actor3Geo_Type',\n",
        "       'Actor3Geo_FullName', 'Randomplace'])\n",
        "\n",
        "dates = training_gdelt['DATEADDED'].tolist()\n",
        "new_dates = []\n",
        "for date in dates:\n",
        "  new_dates.append(str(date)[0:4]+\"-\"+str(date)[4:6]+\"-\"+str(date)[6:8])\n",
        "training_gdelt['dates'] = new_dates\n",
        "training_gdelt['dates'] = pd.to_datetime(training_gdelt['dates'],  format='%Y-%m-%d')\n",
        "training_gdelt = training_gdelt.sort_values(by=\"dates\")\n",
        "get_titles = training_gdelt[training_gdelt['dates']>= \"2019-09-23\"]\n",
        "training_gdelt = training_gdelt.sort_values(by=\"dates\")\n",
        "get_titles = get_titles[get_titles['dates']<= \"2022-07-14\"]\n",
        "listdays = try_df['dates'].tolist()\n",
        "texts_in_dataset = group_titles_by_day(listdays,training_gdelt)\n",
        "texts_by_day = []\n",
        "for textday in texts_in_dataset:\n",
        "  sub_arra = []\n",
        "  i = 0\n",
        "  while i < 10:\n",
        "    if len(textday) < 10:\n",
        "      for x in range(len(textday)):\n",
        "        sub_arra.append(textday[x].replace(\"_\",\" \"))\n",
        "        i += 1\n",
        "      \n",
        "      howmany = abs(len(sub_arra)-10)\n",
        "      for y in range(howmany):\n",
        "        sub_arra.append(\"\")\n",
        "    else:\n",
        "      sub_arra.append(textday[i].replace(\"_\",\" \"))\n",
        "      i += 1\n",
        "  \n",
        "  texts_by_day.append(sub_arra)"
      ],
      "metadata": {
        "id": "epue3_JQhPZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset_text(df,days,flag,texts):\n",
        "  \n",
        "  list_dates = df['dates'].tolist()\n",
        "  quantity_fakes = df['number_fake_news'].tolist()\n",
        "\n",
        "  reached_first = flag\n",
        "  pointer = 0\n",
        "  target_values = []\n",
        "  dataset = []\n",
        "  sizee = 0\n",
        "  new_texts = []\n",
        "  \n",
        "\n",
        "  for date in list_dates:\n",
        "    if reached_first == 1:\n",
        "      textos = []\n",
        "      d = date - timedelta(days=days)\n",
        "      str_d = str(d).split(\" \")[0]\n",
        "\n",
        "      partial_df = df[(df['dates'] <= date) & (df['dates'] >= str_d)]\n",
        "      get_val = df[(df['dates'] == date)]\n",
        "      \n",
        "      target_size = days + 1\n",
        "      \n",
        "      partial_df = partial_df.drop(columns=['dates','number_fake_news'])\n",
        "\n",
        "      if len(partial_df.values.tolist()) == target_size:\n",
        "        dataset.append(partial_df.values.tolist())\n",
        "\n",
        "        for i in range(days+1):\n",
        "          try:\n",
        "            textos.append(texts[pointer])\n",
        "            pointer = pointer + 1\n",
        "       \n",
        "          except:\n",
        "            textos.append(\"\")\n",
        "            pointer = pointer + 1\n",
        "            \n",
        "          \n",
        "        \n",
        "      \n",
        "        sizee += 1\n",
        "\n",
        "        target_values.append(get_val['number_fake_news'].tolist()[0])\n",
        "\n",
        "    else:\n",
        "      if quantity_fakes[pointer] > 1000:\n",
        "        reached_first = 1\n",
        "      pointer += 1\n",
        "\n",
        "  return dataset,target_values,list_dates,texts\n",
        "\n",
        "def prepare_data_with_texts(dataset,target_values,list_days,texts,percent):\n",
        "\n",
        "  train = dataset\n",
        "  train_X = np.asarray(train).astype(np.float32)\n",
        "  train_y = np.asarray(target_values).astype(np.float32)\n",
        "\n",
        "  train_X, train_y = shuffle(train_X, train_y, random_state=0)\n",
        "  aaa, texts = shuffle(train_X, texts, random_state=0)\n",
        "  diff_days = abs(len(train_X)-len(list_days))\n",
        "\n",
        "  list_days = list_days[:len(list_days)-diff_days]\n",
        "\n",
        "  X_test = train_X[int(len(train_X)*percent):] \n",
        "  X_train= train_X[:int(len(train_X)*percent)]\n",
        "\n",
        "  y_test = train_y[int(len(train_y)*percent):]\n",
        "  y_train = train_y[:int(len(train_y)*percent)]\n",
        "\n",
        "  test_texts =  texts[int(len(texts)*percent):]\n",
        "  train_texts = texts[:int(len(texts)*percent)]\n",
        "\n",
        "\n",
        "  return X_test,X_train,y_test,y_train,scaler,test_texts,train_texts"
      ],
      "metadata": {
        "id": "O10YNEgmhaMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "make_linear = []\n",
        "for group in texts_by_day:\n",
        "  for phrase in group:\n",
        "    make_linear.append(phrase)\n",
        "\n",
        "vectorize_layer_for_model = TextVectorization(\n",
        "max_tokens=10000,\n",
        "  output_mode='int',\n",
        "  output_sequence_length=64)\n",
        "\n",
        "vectorize_layer_for_model.adapt(make_linear)"
      ],
      "metadata": {
        "id": "9oZDPEyuhfNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for percent in [0.6,0.9]  \n",
        "  for j in [1,2,4,7]:\n",
        "    days_to_predict = j\n",
        "    epochs = 3000\n",
        "    batch_size = 512\n",
        "    texts_to_input = 1\n",
        "    percentage_training = percent\n",
        "    consistency = 0\n",
        "    test_name = \"xxx\" +str(days_to_predict)+\"_\"+str(epochs)+\"_\"+str(percentage_training)\n",
        "    try_df = count_fakes_in_next_days(to_train1,result,days_to_predict)\n",
        "    try_df = mc.MoveToLast(try_df,'number_fake_news')\n",
        "    try_df = try_df[try_df['dates']>= \"2019-10-01\"]\n",
        "    try_df = try_df[try_df['dates']<= \"2022-07-14\"]\n",
        "    dates_df = try_df['dates']\n",
        "    try_df = try_df.drop(columns=[\"just_dates\",\"dates\"])\n",
        "\n",
        "\n",
        "    train = try_df\n",
        "    scalers={}\n",
        "    for i in try_df.columns:\n",
        "        \n",
        "          scaler = MinMaxScaler(feature_range=(0,1))\n",
        "          s_s = scaler.fit_transform(train[i].values.reshape(-1,1))\n",
        "          s_s=np.reshape(s_s,len(s_s))\n",
        "          scalers['scaler_'+ i] = scaler\n",
        "          train[i]=s_s\n",
        "\n",
        "    train['dates'] = dates_df\n",
        "    if consistency:\n",
        "        \n",
        "      testing_consistency = train[train['dates']>= \"2022-02-01\"]\n",
        "      testing_consistency = testing_consistency[testing_consistency['dates']<= \"2022-02-28\"]\n",
        "      dates_df = testing_consistency['dates']\n",
        "      testing_consistency = testing_consistency.drop(columns=[\"dates\"])\n",
        "        \n",
        "      for i in testing_consistency.columns:\n",
        "          \n",
        "            scaler = scalers['scaler_'+ i]\n",
        "            s_s = scaler.fit_transform(testing_consistency[i].values.reshape(-1,1))\n",
        "            s_s=np.reshape(s_s,len(s_s))\n",
        "            testing_consistency[i]=s_s\n",
        "      testing_consistency['dates'] = dates_df\n",
        "      train = train[~train.dates.isin(testing_consistency.dates)]\n",
        "      testing_consistency = mc.MoveToLast(testing_consistency,'number_fake_news')\n",
        "    for i in [1,2,3,4,5,6,7,8,9,10]:\n",
        "      err = []\n",
        "      errrmse = []\n",
        "      errmse = []\n",
        "      errmae = []\n",
        "      errr2= []\n",
        "      days = i\n",
        "      errrpear = []\n",
        "      errors = []\n",
        "        \n",
        "      for i in range(5):\n",
        "        lowest = 9999\n",
        "        results_experiments = []\n",
        "        \n",
        "        try_df = mc.MoveToLast(try_df,'number_fake_news')\n",
        "        listdays = try_df['dates'].tolist()\n",
        "        texts_in_dataset = group_titles_by_day(listdays,training_gdelt)\n",
        "        texts_by_day = []\n",
        "        \n",
        "        path = \"xxxx\"+test_name+\"/\"+test_name+str(days)+\"/\"\n",
        "        if not os.path.exists(\"xxx\"+test_name+\"/\"):\n",
        "          os.mkdir(\"xxx\"+test_name+\"/\")\n",
        "        if not os.path.exists(\"xxx\"+test_name+\"/\"+test_name+str(days)+\"/\"):\n",
        "          os.mkdir(\"xxx\"+test_name+\"/\"+test_name+str(days)+\"/\")\n",
        "\n",
        "        for textday in texts_in_dataset:\n",
        "          sub_arra = []\n",
        "          i = 0\n",
        "          while i < 10:\n",
        "            if len(textday) < 10:\n",
        "              for x in range(len(textday)):\n",
        "                sub_arra += textday[x]\n",
        "                i += 1\n",
        "              \n",
        "              howmany = abs(len(sub_arra)-10)\n",
        "              for y in range(howmany):\n",
        "                sub_arra.append(\"\")\n",
        "            else:\n",
        "              sub_arra.append(textday[i])\n",
        "              i += 1\n",
        "          \n",
        "          texts_by_day.append(sub_arra)\n",
        "        target_days = i\n",
        "        dataset,target_values,list_days,aa = create_dataset_text(try_df,target_days,1,texts_by_day)\n",
        "\n",
        "        aa = aa[:len(aa)-target_days]\n",
        "\n",
        "        dataset = np.array(dataset)\n",
        "        target_values = np.array(target_values)\n",
        "        X_test,X_train,y_test,y_train,scaler,test_texts,train_texts = prepare_data_with_texts(dataset,target_values,list_days,aa,percentage_training)\n",
        "        train_text_new = np.array(train_texts)\n",
        "        test_text_new = np.array(test_texts)\n",
        "\n",
        "\n",
        "        train_1 = []\n",
        "        train_2 = []\n",
        "        train_3 = []\n",
        "        train_4 = []\n",
        "        train_5 = []\n",
        "        train_6 = []\n",
        "        train_7 = []\n",
        "        train_8 = []\n",
        "        train_9 = []\n",
        "        train_10 = []\n",
        "\n",
        "        test_1 = []\n",
        "        test_2 = []\n",
        "        test_3 = []\n",
        "        test_4 = []\n",
        "        test_5 = []\n",
        "        test_6 = []\n",
        "        test_7 = []\n",
        "        test_8 = []\n",
        "        test_9 = []\n",
        "        test_10 = []\n",
        "\n",
        "        for group in train_text_new:\n",
        "          train_1.append(group[0])\n",
        "          train_2.append(group[1])\n",
        "          train_3.append(group[2])\n",
        "          train_4.append(group[3])\n",
        "          train_5.append(group[4])\n",
        "          train_6.append(group[5])\n",
        "          train_7.append(group[6])\n",
        "          train_8.append(group[7])\n",
        "          train_9.append(group[8])\n",
        "          train_10.append(group[9])\n",
        "\n",
        "        for group in test_text_new:\n",
        "          test_1.append(group[0])\n",
        "          test_2.append(group[1])\n",
        "          test_3.append(group[2])\n",
        "          test_4.append(group[3])\n",
        "          test_5.append(group[4])\n",
        "          test_6.append(group[5])\n",
        "          test_7.append(group[6])\n",
        "          test_8.append(group[7])\n",
        "          test_9.append(group[8])\n",
        "          test_10.append(group[9])\n",
        "\n",
        "        train_1 = np.array(train_1)\n",
        "        train_2 = np.array(train_2)\n",
        "        train_3 = np.array(train_3)\n",
        "        train_4 = np.array(train_4)\n",
        "        train_5 = np.array(train_5)\n",
        "        train_6 = np.array(train_6)\n",
        "        train_7 = np.array(train_7)\n",
        "        train_8 = np.array(train_8)\n",
        "        train_9 = np.array(train_9)\n",
        "        train_10 = np.array(train_10)\n",
        "\n",
        "\n",
        "        test_1 = np.array(test_1)\n",
        "        test_2 = np.array(test_2)\n",
        "        test_3 = np.array(test_3)\n",
        "        test_4 = np.array(test_4)\n",
        "        test_5 = np.array(test_5)\n",
        "        test_6 = np.array(test_6)\n",
        "        test_7 = np.array(test_7)\n",
        "        test_8 = np.array(test_8)\n",
        "        test_9 = np.array(test_9)\n",
        "        test_10 = np.array(test_10)\n",
        "\n",
        "        early_stopping_monitor = EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            min_delta=0,\n",
        "            patience=1000,\n",
        "            verbose=0,\n",
        "            mode='auto',\n",
        "            baseline=None,\n",
        "            restore_best_weights=True\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        model1 = create_model_text(X_train,vectorize_layer_for_model)\n",
        "        history = model1.fit([X_train,train_1,train_2,train_3,train_4,train_5,train_6,train_7,train_8,train_9,train_10], y_train, epochs=3000, batch_size=batch_size,callbacks=[early_stopping_monitor],validation_data=([X_test,test_1,test_2,test_3,test_4,test_5,test_6,test_7,test_8,test_9,test_10], y_test), verbose=0, shuffle=True)\n",
        "        yhat = model1.predict([X_test,test_1,test_2,test_3,test_4,test_5,test_6,test_7,test_8,test_9,test_10])\n",
        "        yhat = yhat.flatten()\n",
        "        y_testw = y_test.flatten()\n",
        "        y_testw = scalers['scaler_number_fake_news'].inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
        "        yhat = scalers['scaler_number_fake_news'].inverse_transform(yhat.reshape(-1, 1)).flatten()\n",
        "        yhat[yhat < 0] = 0\n",
        "        rmse = sqrt(mean_squared_error(y_testw,yhat))\n",
        "        mse = mean_squared_error(y_testw,yhat)\n",
        "        mae = np.mean(np.abs(y_testw - yhat))\n",
        "        r2 = r2_score(y_testw, yhat)\n",
        "        cor = median_abs_deviation(y_testw - yhat) \n",
        "        with open(\"xxxx\"+test_name+\"/\"+test_name+str(days)+'/'+str(i)+\"savingrest.pkl\",'wb') as f:\n",
        "            pickle.dump(yhat, f)\n",
        "        with open(\"xxxx\"+test_name+\"/\"+test_name+str(days)+'/'+str(i)+\"other.pkl\",'wb') as f:\n",
        "            pickle.dump(y_testw, f) \n",
        "        errrmse.append(rmse)\n",
        "        errmae.append(mae)\n",
        "        errmse.append(mse)\n",
        "        errrpear.append(cor)\n",
        "        errr2.append(r2)\n",
        "\n",
        "        np.savetxt(\"xxxx\"+test_name+\"/\"+test_name+str(days)+\"/\"+test_name+\"round\"+str(i)+\".csv\", [rmse,mae,mse,cor,r2], delimiter=\",\")\n",
        "\n",
        "      mediaremse = statistics.mean(errrmse)\n",
        "      mediamae = statistics.mean(errmae)\n",
        "      mediamse = statistics.mean(errmse)\n",
        "      mediamr2 = statistics.mean(errr2)\n",
        "      meadipear = statistics.mean(errrpear)\n",
        "      errors.append([mediaremse,mediamae,mediamse,mediamr2,meadipear])\n",
        "      arra = np.array(errors)\n",
        "      np.savetxt(\"xxxx\"+test_name+\"/\"+test_name+str(days)+\"/\"+test_name+\"total.csv\", arra, delimiter=\",\")\n",
        "    "
      ],
      "metadata": {
        "id": "HvfuEBO1iDin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h82mdahb2jR1"
      },
      "outputs": [],
      "source": [
        "to_train1 = pd.read_csv('xxxx.csv')\n",
        "result = pd.read_csv('xxx.csv')\n",
        "try_df = pd.read_csv('/content/drive/MyDrive/Luis/experiments_prediction_fake_news/data/try_df.csv')\n",
        "prueba = to_train1.drop(columns=[ 'mean_neutral', 'max_neutral', 'min_neutral', 'count_neutral', 'mean_negative', 'max_negative', 'min_negative', 'mean_positive', 'max_positive', 'min_positive', 'max_IsRootEvent', 'min_IsRootEvent',])\n",
        "prueba['just_dates'] = pd.to_datetime(prueba['just_dates'])\n",
        "prueba['dates'] = pd.to_datetime(prueba['dates'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "uZvWBVXv5FIM"
      },
      "outputs": [],
      "source": [
        "for percentage_train in [0.6,0.9]:\n",
        "  for j in [1,2,4,7]:\n",
        "    days_to_predict = j\n",
        "    epochs = 3000\n",
        "    batch_size = 512\n",
        "    texts_to_input = 0\n",
        "    percentage_training = percentage_train\n",
        "    consistency = 1\n",
        "    test_name = \"xxxxx\" +str(days_to_predict)+\"_\"+str(epochs)+\"_\"+str(batch_size)+\"_\"+str(percentage_training)\n",
        "    try_df = count_fakes_in_next_days(prueba,result,days_to_predict)\n",
        "    try_df = mc.MoveToLast(try_df,'number_fake_news')\n",
        "    try_df = try_df[try_df['dates']>= \"2019-10-01\"]\n",
        "    try_df = try_df[try_df['dates']<= \"2022-07-14\"]\n",
        "    dates_df = try_df['dates']\n",
        "    try_df = try_df.drop(columns=[\"just_dates\",\"dates\"])\n",
        "\n",
        "    train = try_df\n",
        "    scalers={}\n",
        "    for i in try_df.columns:\n",
        "     \n",
        "          scaler = MinMaxScaler()\n",
        "          s_s = scaler.fit_transform(train[i].values.reshape(-1,1))\n",
        "          s_s=np.reshape(s_s,len(s_s))\n",
        "          scalers['scaler_'+ i] = scaler\n",
        "          train[i]=s_s\n",
        "\n",
        "    train['dates'] = dates_df\n",
        "    if consistency:\n",
        "      \n",
        "      testing_consistency = train[train['dates']>= \"2022-03-01\"]\n",
        "      testing_consistency = testing_consistency[testing_consistency['dates']<= \"2022-03-30\"]\n",
        "      dates_df = testing_consistency['dates']\n",
        "      testing_consistency = testing_consistency.drop(columns=[\"dates\"])\n",
        "      \n",
        "\n",
        "      testing_consistency['dates'] = dates_df\n",
        "      train = train[~train.dates.isin(testing_consistency.dates)]\n",
        "      testing_consistency = mc.MoveToLast(testing_consistency,'number_fake_news')\n",
        "\n",
        "    results_experiments = []\n",
        "    errors = []\n",
        "    try_df = mc.MoveToLast(try_df,'number_fake_news')\n",
        "    \n",
        "    for days in [1,2,3,4,5,6,7,8,9,10]:\n",
        "      err = []\n",
        "      errrmse = []\n",
        "      errmse = []\n",
        "      errmae = []\n",
        "      errr2= []\n",
        "      errrpear = []\n",
        "      partial_results = []\n",
        "      ys = None\n",
        "      for i in range(5):\n",
        "  \n",
        "        path = \"xxx\"+test_name+\"/\"+test_name+str(days)+\"/\"\n",
        "        if not os.path.exists(\"xxx\"+test_name+\"/\"):\n",
        "          os.mkdir(\"xxx\"+test_name+\"/\")\n",
        "\n",
        "        tf.keras.utils.set_random_seed(i) \n",
        "        dataset,target_values,list_days = create_dataset_target_days(train,days)\n",
        "        dataset = np.array(dataset)\n",
        "        target_values = np.array(target_values)\n",
        "      \n",
        "        X_test,X_train,y_test,y_train,scaler = prepare_data(dataset,target_values,list_days,percentage_training,i)\n",
        "\n",
        "        if consistency:\n",
        "          dataset_check,target_values_check,list_days = create_dataset_target_days(testing_consistency,days)\n",
        "          dataset_check = np.array(dataset_check)\n",
        "          target_values_check = np.array(target_values_check)\n",
        "          consistency_x,consistency_y = prepare_data_consistency(dataset_check,target_values_check,scaler)\n",
        "        ys = y_test\n",
        "\n",
        "        if not os.path.exists(path):\n",
        "          os.mkdir(path)\n",
        "      \n",
        "\n",
        "        model = autoencoder(X_train)\n",
        "        early_stopping_monitor = EarlyStopping(\n",
        "      monitor='val_loss',\n",
        "      min_delta=0,\n",
        "      patience=1000,\n",
        "      verbose=0,\n",
        "      mode='auto',\n",
        "      baseline=None,\n",
        "      restore_best_weights=True\n",
        "  )\n",
        "\n",
        "        if consistency == 1:\n",
        "          history = model.fit(X_train, y_train, epochs=epochs, validation_data=(consistency_x,consistency_y ), batch_size=batch_size, verbose=0,callbacks=[early_stopping_monitor], shuffle=True)\n",
        "        else:\n",
        "          history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test),callbacks=[early_stopping_monitor], verbose=0, shuffle=True)\n",
        "\n",
        "      \n",
        "        if consistency:\n",
        "          yhat = model.predict(consistency_x)\n",
        "          yhat = yhat.flatten()\n",
        "          consistency_y = consistency_y.flatten()\n",
        "          consistency_y = scalers['scaler_number_fake_news'].inverse_transform(consistency_y.reshape(-1, 1)).flatten()\n",
        "          yhat = scalers['scaler_number_fake_news'].inverse_transform(yhat.reshape(-1, 1)).flatten()\n",
        "          yhat[yhat < 0] = 0\n",
        "          rmse = sqrt(mean_squared_error(consistency_y,yhat))\n",
        "          mse = mean_squared_error(consistency_y,yhat)\n",
        "          mae = np.mean(np.abs(consistency_y - yhat))\n",
        "          r2 = r2_score(consistency_y, yhat)  \n",
        "          cor = np.corrcoef(consistency_y, yhat)[0][1] \n",
        "          \n",
        "\n",
        "          with open(\"xxxx\"+test_name+\"/\"+test_name+str(days)+'/'+str(i)+\"xxx.pkl\",'wb') as f:\n",
        "            pickle.dump(yhat, f)\n",
        "          with open(\"xxx/\"+test_name+\"/\"+test_name+str(days)+'/'+str(i)+\"xxx.pkl\",'wb') as f:\n",
        "            pickle.dump(consistency_y, f) \n",
        "\n",
        "        else:\n",
        "\n",
        "          yhat = model.predict(X_test)\n",
        "          yhat = yhat.flatten()\n",
        "          y_test = y_test.flatten()\n",
        "          y_test = scalers['scaler_number_fake_news'].inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
        "          yhat = scalers['scaler_number_fake_news'].inverse_transform(yhat.reshape(-1, 1)).flatten()\n",
        "          yhat[yhat < 0] = 0\n",
        "      \n",
        "          rmse = sqrt(mean_squared_error(y_test,yhat))\n",
        "          mse = mean_squared_error(y_test,yhat)\n",
        "          mae = np.mean(np.abs(y_test - yhat))\n",
        "          r2 = r2_score(y_test, yhat)\n",
        "\n",
        "\n",
        "          with open(\"xxxx\"+test_name+\"/\"+test_name+str(days)+'/'+str(i)+\"xxxx.pkl\",'wb') as f:\n",
        "            pickle.dump(yhat, f)\n",
        "          with open(\"xxxx\"+test_name+\"/\"+test_name+str(days)+'/'+str(i)+\"xxxx.pkl\",'wb') as f:\n",
        "            pickle.dump(y_test, f) \n",
        "\n",
        "        \n",
        "\n",
        "        partial_results.append(yhat[0])\n",
        "\n",
        "        errrmse.append(rmse)\n",
        "        errmae.append(mae)\n",
        "        errmse.append(mse)\n",
        "        errrpear.append(cor)\n",
        "        errr2.append(r2)\n",
        "\n",
        "        \n",
        "        plt.figure(figsize=(80,10))\n",
        "        if consistency:\n",
        "          plt.plot(yhat,label='Prediction',alpha=0.8, linestyle=\"dotted\")\n",
        "          plt.plot(consistency_y,label='Real')\n",
        "        else:\n",
        "          plt.plot(yhat,label='Prediction',alpha=0.8, linestyle=\"dotted\")\n",
        "          plt.plot(y_test,label='Real')\n",
        "\n",
        "        plt.legend(loc=\"upper left\")\n",
        "        plt.savefig(\"xxxx\"+test_name+\"/\"+test_name+str(days)+'/'+str(i)+\"_\"+str(days)+'.png')\n",
        "      plt.show()\n",
        "\n",
        "\n",
        "      plt.figure(figsize=(80,10))\n",
        "      if consistency:\n",
        "        plt.plot(consistency_y,label='Real',linewidth=2.0)\n",
        "      else:\n",
        "        plt.plot(y_test,label='Real',linewidth=2.0)\n",
        "\n",
        "      avgs = np.squeeze(np.average(partial_results, axis=0))\n",
        "      plt.plot(avgs,label='Avg predictions',alpha=0.6)\n",
        "      mins = np.squeeze(np.min(partial_results, axis=0))\n",
        "      maxs = np.squeeze(np.max(partial_results, axis=0))\n",
        "      xs = []\n",
        "      if consistency:\n",
        "        for i in range(len(consistency_y)):\n",
        "          xs.append(i)\n",
        "      else:\n",
        "        for i in range(len(y_test)):\n",
        "          xs.append(i)\n",
        "\n",
        "      plt.plot(mins,alpha=0.5)\n",
        "      plt.plot(maxs,alpha=0.5)\n",
        "      plt.fill_between(xs,mins,maxs,alpha=0.2,interpolate=True,color=\"grey\")\n",
        "      plt.legend(loc=\"upper left\")\n",
        "      plt.savefig(\"xxxx\"+test_name+\"/\"+test_name+str(days)+'/'+\"TOTAL\"+str(days)+'.png')\n",
        "\n",
        "      results_experiments.append(partial_results)\n",
        "      mediaremse = statistics.mean(errrmse)\n",
        "      mediamae = statistics.mean(errmae)\n",
        "      mediamse = statistics.mean(errmse)\n",
        "      mediamr2 = statistics.mean(errr2)\n",
        "      meadipear = statistics.mean(errrpear)\n",
        "  \n",
        "      errors.append([mediaremse,mediamae,mediamse,mediamr2,meadipear])\n",
        "      arra = np.array(errors)\n",
        "      np.savetxt(\"xxxx\"+test_name+\"/\"+test_name+str(days)+\"/\"+test_name+\".csv\", arra, delimiter=\",\")\n",
        "\n",
        "    arra = np.array(errors)\n",
        "    np.savetxt(\"xxx\"+test_name+\"/\"+test_name+str(days)+\"/\"+test_name+\".csv\", arra, delimiter=\",\")\n",
        "    np.savetxt(\"/xxxx/\"+test_name+\".csv\", arra, delimiter=\",\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predicion on a date"
      ],
      "metadata": {
        "id": "-xovowI3juQD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_fakes_in_next_days_prediction(df,df_maldita,days):\n",
        "  dates = df['dates'].tolist()\n",
        "  len_dates = []\n",
        "  predictions = []\n",
        "\n",
        "  for date in dates:  \n",
        "    grupo = []\n",
        "    for dia in range(days):\n",
        "      objetivo = dia\n",
        "      d = date + timedelta(days=objetivo)\n",
        "      str_d = str(d).split(\" \")[0]\n",
        "      original_date = str(date).split(\" \")[0]\n",
        "      testing_week = df_maldita[(df_maldita['just_date'] == str_d)]\n",
        "      grupo.append(len(testing_week))\n",
        "    \n",
        "    predictions.append(grupo)\n",
        "  return predictions\n",
        "\n",
        "\n",
        "def count_fakes_in_next_days_prediction_co(df,df_maldita,days,used):\n",
        "  dates = df['dates'].tolist()\n",
        "  len_dates = []\n",
        "  predictions = []\n",
        "  for date in dates:\n",
        "  \n",
        "    grupo = []\n",
        "    for dia in range(days):\n",
        "      objetivo = dia+used+1\n",
        "      d = date + timedelta(days=objetivo)\n",
        "      str_d = str(d).split(\" \")[0]\n",
        "      original_date = str(date).split(\" \")[0]\n",
        "      testing_week = df_maldita[(df_maldita['just_date'] == str_d)]\n",
        "      grupo.append(len(testing_week)) \n",
        "    predictions.append(grupo)\n",
        "  return predictions"
      ],
      "metadata": {
        "id": "O6dY6-f5kA9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for prediction in [2,4,7]:\n",
        "  errors = []\n",
        "  prueba = to_train1[to_train1['dates']>= \"2019-10-01\"]\n",
        "  prueba = prueba[prueba['dates']<= \"2022-07-14\"]\n",
        "  prueba = prueba.drop(columns=[ 'mean_neutral', 'max_neutral', 'min_neutral', 'count_neutral', 'mean_negative', 'max_negative', 'min_negative', 'mean_positive', 'max_positive', 'min_positive', 'max_IsRootEvent', 'min_IsRootEvent'])\n",
        "  dates_df = prueba['dates']\n",
        "  days_to_predict = prediction\n",
        "  epochs = 3000\n",
        "  batch_size = 1000\n",
        "  texts_to_input = 0\n",
        "  percentage_training = 1\n",
        "  consistency = 1\n",
        "  seed = 1\n",
        "  for days_window in [4,9,14,19]:\n",
        "    used = days_window\n",
        "    test_name = \"xxxx\" +str(days_to_predict)+\"_\"+str(epochs)+\"_\"+str(batch_size)+\"_\"+str(percentage_training)\n",
        "    path = \"xxxx\"+test_name+\"/\"+test_name+str(days_window)+\"/\"\n",
        "    if not os.path.exists(\"xxxx\"+test_name+\"/\"):\n",
        "          os.mkdir(\"xxx\"+test_name+\"/\")\n",
        "          os.mkdir(\"xxx\"+test_name+\"/\"+test_name+str(days_window)+\"/\")\n",
        "    if not os.path.exists(\"xxxx\"+test_name+\"/\"+test_name+str(days_window)+\"/\"):\n",
        "        \n",
        "          os.mkdir(\"/xxx\"+test_name+\"/\"+test_name+str(days_window)+\"/\")\n",
        "     \n",
        "    lista_dias = count_fakes_in_next_days_prediction_co(prueba,result,days_to_predict,used)\n",
        "    prueba['dates'] = dates_df\n",
        "    prueba['just_dates'] = dates_df\n",
        "\n",
        "    prueba = count_fakes_in_next_days(prueba,result,1)\n",
        "    prueba = prueba.drop(columns=[\"just_dates\",\"dates\"])\n",
        "\n",
        "    train = prueba\n",
        "    scalers={}\n",
        "    for i in prueba.columns:\n",
        "      if not 'scaler_'+ i == \"scaler_number_fake_news\":\n",
        "          scaler = MinMaxScaler()\n",
        "          s_s = scaler.fit_transform(train[i].values.reshape(-1,1))\n",
        "          s_s=np.reshape(s_s,len(s_s))\n",
        "          scalers['scaler_'+ i] = scaler\n",
        "          train[i]=s_s\n",
        "    train['dates'] = dates_df\n",
        "    if consistency: \n",
        "      datetimenew = datetime.strptime(\"2022-03-30\", '%Y-%m-%d')\n",
        "      end_date = datetimenew \n",
        "      datetimenew = datetime.strptime(\"2022-03-01\", '%Y-%m-%d')\n",
        "      start = datetimenew\n",
        "\n",
        "      testing_consistency = train[train['dates']>= start]\n",
        "      end_datestr = str(end_date).split(\" \")\n",
        "      testing_consistency = testing_consistency[testing_consistency['dates']<= end_datestr[0]]\n",
        "\n",
        "      train = train[~train.dates.isin(testing_consistency.dates)]\n",
        "      lista_dias_consistencia = count_fakes_in_next_days_prediction_co(testing_consistency,result,days_to_predict,used)\n",
        "\n",
        "\n",
        "    dataset,list_days = create_dataset_target_days(train,used)\n",
        "    dataset = np.array(dataset)\n",
        "    dif = abs(len(dataset) - len(lista_dias))\n",
        "    lista_dias = lista_dias[dif:]\n",
        "    target_values = lista_dias\n",
        "    X_test,X_train,y_test,y_train,scaler = prepare_data(dataset,target_values,list_days,percentage_training,seed)\n",
        "    X_train = np.array(X_train)\n",
        "\n",
        "\n",
        "    if consistency:\n",
        "          dataset_check,list_days_con = create_dataset_target_days(testing_consistency,used)\n",
        "          dataset_check = np.array(dataset_check)\n",
        "          lista_dias_consistencia = np.array(lista_dias_consistencia)\n",
        "          dif = abs(len(dataset_check) - len(lista_dias_consistencia))\n",
        "          lista_dias_consistencia = lista_dias_consistencia[dif:]\n",
        "          X_test_con,X_train_con,y_test_con,y_train_con = prepare_data_consistency(dataset_check,lista_dias_consistencia,list_days,1)\n",
        "\n",
        "\n",
        "    rmse_to = []\n",
        "    mse_to = []\n",
        "    mae_to = []\n",
        "    r2_to = []\n",
        "    for round in range(5):\n",
        "      tf.keras.utils.set_random_seed(round)\n",
        "      model = create_autoencoders(X_train,y_train)\n",
        "      early_stopping_monitor = EarlyStopping(\n",
        "          monitor='val_loss',\n",
        "          min_delta=0,\n",
        "          patience=1000,\n",
        "          verbose=0,\n",
        "          mode='auto',\n",
        "          baseline=None,\n",
        "          restore_best_weights=True\n",
        "      )\n",
        "\n",
        "      if consistency == 1:\n",
        "          history = model.fit(X_train, y_train, epochs=epochs, validation_data=( X_train_con,y_train_con ), batch_size=1000, verbose=0,callbacks=[early_stopping_monitor], shuffle=True)\n",
        "      else:\n",
        "          history = model.fit(X_train, y_train, epochs=epochs, batch_size=1000, validation_data=(X_test, y_test),callbacks=[early_stopping_monitor], verbose=0, shuffle=True)\n",
        "\n",
        "\n",
        "      yhat = model.predict(X_train_con)\n",
        "      predic = yhat.flatten()\n",
        "      ground = y_train_con.flatten()\n",
        "      rmse = sqrt(mean_squared_error(ground,predic))\n",
        "      mse = mean_squared_error(ground,predic)\n",
        "      mae = np.mean(np.abs(ground - predic))\n",
        "      r2 = r2_score(ground, predic)  \n",
        "      np.savetxt(\"xxxx\"+test_name+\"/\"+test_name+str(days_window)+\"/\"+str(round)+\"xx.csv\", [rmse,mse,mae,r2], delimiter=\",\")\n",
        "\n",
        "      rmse_to.append(rmse)\n",
        "      mse_to.append(mse)\n",
        "      mae_to.append(mae)\n",
        "      r2_to.append(r2)\n",
        "      save = []\n",
        "      save.append([rmse,mse,mae,r2])\n",
        "      arra = np.array(save)\n",
        "      np.savetxt(\"xxxx\"+test_name+\"/\"+test_name+str(days_window)+\"/\"+str(round)+\"xxx.csv\", arra, delimiter=\",\")\n",
        "\n",
        "    mediaremse = statistics.mean(rmse_to)\n",
        "    mediamae = statistics.mean(mae_to)\n",
        "    mediamse = statistics.mean(mse_to)\n",
        "    mediamr2 = statistics.mean(r2_to)\n",
        "    \n",
        "    errors.append([mediaremse,mediamae,mediamse,mediamr2])\n",
        "    arra = np.array(errors)\n",
        "    np.savetxt(\"xxxx\"+test_name+\"/\"+test_name+str(days_window)+\"/\"+\"xxx.csv\", arra, delimiter=\",\")\n"
      ],
      "metadata": {
        "id": "bee6GU4Pjybd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for prediction in [2,4,7]:\n",
        "  errors = []\n",
        "  prueba = to_train1[to_train1['dates']>= \"2019-10-01\"]\n",
        "  prueba = prueba[prueba['dates']<= \"2022-07-14\"]\n",
        "  prueba = prueba.drop(columns=[ 'mean_neutral', 'max_neutral', 'min_neutral', 'count_neutral', 'mean_negative', 'max_negative', 'min_negative', 'mean_positive', 'max_positive', 'min_positive', 'max_IsRootEvent', 'min_IsRootEvent'])\n",
        "  dates_df = prueba['dates']\n",
        "  days_to_predict = prediction\n",
        "  epochs = 3000\n",
        "  batch_size = 512\n",
        "  texts_to_input = 0\n",
        "  percentage_training = 1\n",
        "  consistency = 1\n",
        "\n",
        "  for days_window in [4,9,14,19]:\n",
        "    try_df = count_fakes_in_next_days(to_train1,result,days_to_predict)\n",
        "    try_df = mc.MoveToLast(try_df,'number_fake_news')\n",
        "    try_df = try_df[try_df['dates']>= \"2019-10-01\"]\n",
        "    try_df = try_df[try_df['dates']<= \"2022-07-14\"]\n",
        "    dates_df = try_df['dates']\n",
        "    test_name = \"marzo_contex\" +str(days_to_predict)+\"_\"+str(epochs)+\"_\"+str(batch_size)+\"_\"+str(percentage_training)\n",
        "\n",
        "    try_df = try_df.drop(columns=[\"just_dates\",\"dates\"])\n",
        "\n",
        "    train = try_df\n",
        "    scalers={}\n",
        "    for i in try_df.columns:\n",
        "        \n",
        "          scaler = MinMaxScaler(feature_range=(0,1))\n",
        "          s_s = scaler.fit_transform(train[i].values.reshape(-1,1))\n",
        "          s_s=np.reshape(s_s,len(s_s))\n",
        "          scalers['scaler_'+ i] = scaler\n",
        "          train[i]=s_s\n",
        "\n",
        "    train['dates'] = dates_df\n",
        "    if consistency:\n",
        "      given_date = '2022-03-01'\n",
        "      date_format = '%Y-%m-%d'\n",
        "\n",
        "      dtObj = datetime.strptime(given_date, date_format)\n",
        "      d = dtObj- timedelta(days=days_window)\n",
        "      testing_consistency = train[train['dates']>= d]\n",
        "\n",
        "      testing_consistency = testing_consistency[testing_consistency['dates']<= \"2022-03-30\"]\n",
        "      dates_df = testing_consistency['dates']\n",
        "      testing_consistency = testing_consistency.drop(columns=[\"dates\"])\n",
        "      testing_consistency['dates'] = dates_df\n",
        "      train = train[~train.dates.isin(testing_consistency.dates)]\n",
        "      testing_consistency = mc.MoveToLast(testing_consistency,'number_fake_news')      \n",
        "\n",
        "\n",
        "\n",
        "    err = []\n",
        "    errrmse = []\n",
        "    errmse = []\n",
        "    errmae = []\n",
        "    errr2= []\n",
        "    days = days_window\n",
        "    errrpear = []\n",
        "    errors = []\n",
        "    used = days_window\n",
        "    for i in range(5):\n",
        "      results_experiments = []\n",
        "      try_df = mc.MoveToLast(train,'number_fake_news')\n",
        "      listdays = try_df['dates'].tolist()\n",
        "      lista_dias = count_fakes_in_next_days_prediction_co(try_df,result,days_to_predict,used)\n",
        "\n",
        "      dias_tr = train['dates'].to_list()\n",
        "      df_filtered = training_gdelt[training_gdelt['dates'].isin(dias_tr)]\n",
        "      texts_in_dataset = group_titles_by_day(listdays,df_filtered)\n",
        "      texts_by_day = []\n",
        "      path = \"xxx\"+test_name+\"/\"+test_name+str(days)+\"/\"\n",
        "      if not os.path.exists(\"xxxx\"+test_name+\"/\"):\n",
        "        os.mkdir(\"xxx\"+test_name+\"/\")\n",
        "      if not os.path.exists(\"xxx\"+test_name+\"/\"+test_name+str(days)+\"/\"):\n",
        "        os.mkdir(\"xxx\"+test_name+\"/\"+test_name+str(days)+\"/\")\n",
        "      print(\"saving in: \"+ path)\n",
        "      for textday in texts_in_dataset:\n",
        "        sub_arra = []\n",
        "        i = 0\n",
        "        while i < 10:\n",
        "          if len(textday) < 10:\n",
        "            for x in range(len(textday)):\n",
        "              sub_arra += textday[x]\n",
        "              i += 1\n",
        "            \n",
        "            howmany = abs(len(sub_arra)-10)\n",
        "            for y in range(howmany):\n",
        "              sub_arra.append(\"\")\n",
        "          else:\n",
        "            sub_arra.append(textday[i])\n",
        "            i += 1\n",
        "        \n",
        "        texts_by_day.append(sub_arra)\n",
        "      \n",
        "\n",
        "\n",
        "      dias_tr = testing_consistency['dates'].to_list()\n",
        "      df_filtered2 = training_gdelt[training_gdelt['dates'].isin(dias_tr)]\n",
        "      listdays = testing_consistency['dates'].tolist()\n",
        "      texts_in_dataset2 = group_titles_by_day(listdays,df_filtered2)\n",
        "      texts_by_day2 = []\n",
        "      \n",
        "       \n",
        "      path = \"xxx\"+test_name+\"/\"+test_name+str(days)+\"/\"\n",
        "      if not os.path.exists(\"xxx\"+test_name+\"/\"):\n",
        "        os.mkdir(\"xxx\"+test_name+\"/\")\n",
        "      if not os.path.exists(\"xxx\"+test_name+\"/\"+test_name+str(days)+\"/\"):\n",
        "        os.mkdir(\"xxx\"+test_name+\"/\"+test_name+str(days)+\"/\")\n",
        "      print(\"saving in: \"+ path)\n",
        "      for textday in texts_in_dataset2:\n",
        "        sub_arra = []\n",
        "        i = 0\n",
        "        while i < 10:\n",
        "          if len(textday) < 10:\n",
        "            for x in range(len(textday)):\n",
        "              sub_arra += textday[x]\n",
        "              i += 1\n",
        "            \n",
        "            howmany = abs(len(sub_arra)-10)\n",
        "            for y in range(howmany):\n",
        "              sub_arra.append(\"\")\n",
        "          else:\n",
        "            sub_arra.append(textday[i])\n",
        "            i += 1\n",
        "        \n",
        "        texts_by_day2.append(sub_arra)\n",
        "      lista_dias2 = count_fakes_in_next_days_prediction_co(testing_consistency,result,days_to_predict,used)\n",
        "      target_days = i\n",
        "      dataset,target_values,list_days,aa = create_dataset_text(try_df,target_days,1,texts_by_day)\n",
        "      dataset2,target_values2,list_days2,aa2 = create_dataset_text(testing_consistency,target_days,1,texts_by_day2)\n",
        "      difs = abs(len(dataset)-len(aa))\n",
        "      aa = aa[:len(aa)-difs]\n",
        "      aa2 = aa2[:len(aa2)-target_days]\n",
        "      dif = abs(len(dataset) - len(lista_dias))\n",
        "      lista_dias = lista_dias[:len(lista_dias)-dif]\n",
        "      dif = abs(len(dataset2) - len(lista_dias2))\n",
        "      lista_dias2 = lista_dias2[:len(lista_dias2)-dif]\n",
        "\n",
        "      dataset = np.array(dataset)\n",
        "      target_values = np.array(target_values)\n",
        "      X_test,X_train,y_test,y_train,scaler,test_texts,train_texts = prepare_data_with_texts(dataset,lista_dias,list_days,aa,percentage_training)\n",
        "      dataset2 = np.array(dataset2)\n",
        "      target_values2 = np.array(target_values2)\n",
        "      X_test2,X_train2,y_test2,y_train2,scaler2,test_texts2,train_texts2 = prepare_data_with_texts_no(dataset2,lista_dias2,list_days2,aa2,percentage_training)\n",
        "\n",
        "      train_text_new = np.array(train_texts)\n",
        "      test_text_new = np.array(train_texts2)\n",
        "      train_1 = []\n",
        "      train_2 = []\n",
        "      train_3 = []\n",
        "      train_4 = []\n",
        "      train_5 = []\n",
        "      train_6 = []\n",
        "      train_7 = []\n",
        "      train_8 = []\n",
        "      train_9 = []\n",
        "      train_10 = []\n",
        "\n",
        "      test_1 = []\n",
        "      test_2 = []\n",
        "      test_3 = []\n",
        "      test_4 = []\n",
        "      test_5 = []\n",
        "      test_6 = []\n",
        "      test_7 = []\n",
        "      test_8 = []\n",
        "      test_9 = []\n",
        "      test_10 = []\n",
        "\n",
        "      for group in train_text_new:\n",
        "        train_1.append(group[0])\n",
        "        train_2.append(group[1])\n",
        "        train_3.append(group[2])\n",
        "        train_4.append(group[3])\n",
        "        train_5.append(group[4])\n",
        "        train_6.append(group[5])\n",
        "        train_7.append(group[6])\n",
        "        train_8.append(group[7])\n",
        "        train_9.append(group[8])\n",
        "        train_10.append(group[9])\n",
        "\n",
        "      for group in test_text_new:\n",
        "        test_1.append(group[0])\n",
        "        test_2.append(group[1])\n",
        "        test_3.append(group[2])\n",
        "        test_4.append(group[3])\n",
        "        test_5.append(group[4])\n",
        "        test_6.append(group[5])\n",
        "        test_7.append(group[6])\n",
        "        test_8.append(group[7])\n",
        "        test_9.append(group[8])\n",
        "        test_10.append(group[9])\n",
        "\n",
        "      train_1 = np.array(train_1)\n",
        "      train_2 = np.array(train_2)\n",
        "      train_3 = np.array(train_3)\n",
        "      train_4 = np.array(train_4)\n",
        "      train_5 = np.array(train_5)\n",
        "      train_6 = np.array(train_6)\n",
        "      train_7 = np.array(train_7)\n",
        "      train_8 = np.array(train_8)\n",
        "      train_9 = np.array(train_9)\n",
        "      train_10 = np.array(train_10)\n",
        "\n",
        "\n",
        "      test_1 = np.array(test_1)\n",
        "      test_2 = np.array(test_2)\n",
        "      test_3 = np.array(test_3)\n",
        "      test_4 = np.array(test_4)\n",
        "      test_5 = np.array(test_5)\n",
        "      test_6 = np.array(test_6)\n",
        "      test_7 = np.array(test_7)\n",
        "      test_8 = np.array(test_8)\n",
        "      test_9 = np.array(test_9)\n",
        "      test_10 = np.array(test_10)\n",
        "\n",
        "      early_stopping_monitor = EarlyStopping(\n",
        "          monitor='val_loss',\n",
        "          min_delta=0,\n",
        "          patience=1000,\n",
        "          verbose=0,\n",
        "          mode='auto',\n",
        "          baseline=None,\n",
        "          restore_best_weights=True\n",
        "      )\n",
        "      model1 = create_model_text(X_train,vectorize_layer_for_model,y_train)\n",
        "      history = model1.fit([X_train,train_1,train_2,train_3,train_4,train_5,train_6,train_7,train_8,train_9,train_10], y_train, epochs=3000, batch_size=batch_size,callbacks=[early_stopping_monitor],validation_data=([X_train2,test_1,test_2,test_3,test_4,test_5,test_6,test_7,test_8,test_9,test_10], y_train2), verbose=0, shuffle=True)\n",
        "      yhat = model1.predict([X_train2,test_1,test_2,test_3,test_4,test_5,test_6,test_7,test_8,test_9,test_10])\n",
        "      yhat = yhat.flatten()\n",
        "      y_testw = y_train2.flatten()\n",
        "\n",
        "      yhat[yhat < 0] = 0\n",
        "      rmse = sqrt(mean_squared_error(y_testw,yhat))\n",
        "      mse = mean_squared_error(y_testw,yhat)\n",
        "      mae = np.mean(np.abs(y_testw - yhat))\n",
        "      r2 = r2_score(y_testw, yhat)\n",
        "      cor = np.corrcoef(y_testw, yhat)[0][1] \n",
        "\n",
        "      errrmse.append(rmse)\n",
        "      errmae.append(mae)\n",
        "      errmse.append(mse)\n",
        "      errrpear.append(cor)\n",
        "      errr2.append(r2)\n",
        "\n",
        "      np.savetxt(\"xxxx\"+test_name+\"/\"+test_name+str(days)+\"/\"+test_name+\"round\"+str(i)+\".csv\", [rmse,mae,mse,cor,r2], delimiter=\",\")\n",
        "\n",
        "    mediaremse = statistics.mean(errrmse)\n",
        "    mediamae = statistics.mean(errmae)\n",
        "    mediamse = statistics.mean(errmse)\n",
        "    mediamr2 = statistics.mean(errr2)\n",
        "    meadipear = statistics.mean(errrpear)\n",
        "    errors.append([mediaremse,mediamae,mediamse,mediamr2,meadipear])\n",
        "    arra = np.array(errors)\n",
        "    np.savetxt(\"xxxx\"+test_name+\"/\"+test_name+str(days)+\"/\"+test_name+\"total.csv\", arra, delimiter=\",\")\n",
        "\n",
        "\n",
        "\n",
        "       \n",
        "\n"
      ],
      "metadata": {
        "id": "JOKlnSSBkiGX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "1dGbqkPOsGdU",
        "rTrKDgofsk0J"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}